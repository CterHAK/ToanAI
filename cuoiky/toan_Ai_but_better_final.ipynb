{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TbU9rUyex6jA",
        "outputId": "b29f4857-a9eb-4c28-ab5e-887d8a5f931f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.15.2)\n",
            "Collecting PyWavelets\n",
            "  Downloading pywavelets-1.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (1.4.2)\n",
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.11/dist-packages (4.5.0)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (3.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl) (2.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Downloading pywavelets-1.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyWavelets\n",
            "Successfully installed PyWavelets-1.8.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas numpy matplotlib scipy PyWavelets scikit-learn joblib lightgbm openpyxl"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.fft import fft\n",
        "import pywt\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import joblib\n",
        "from scipy.stats import entropy\n",
        "import zipfile\n",
        "import tempfile\n",
        "from collections import defaultdict\n",
        "import json\n",
        "import lightgbm as lgb\n",
        "import re\n",
        "import warnings\n",
        "\n",
        "# Tắt các cảnh báo không cần thiết\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)  # Tắt cảnh báo wavelet\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)  # Tắt cảnh báo tương lai từ sklearn/seaborn\n",
        "\n",
        "# Tạo thư mục output ngay đầu mã\n",
        "output_dir = 'model_outputs'\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "# Bước 1: Giải nén file ZIP và đọc dữ liệu\n",
        "zip_file = 'Wind Turbine Blades Fault Diagnosis based on Vibration Dataset Analysis.zip'\n",
        "\n",
        "# File cấu hình ánh xạ tên file với trạng thái và tốc độ gió\n",
        "config_file = 'file_config.json'\n",
        "file_config = {\n",
        "    'H-for-Vw=5.0.csv': {'state': 'healthy', 'wind_speed': 5.0},\n",
        "    'Crack Fault-Vw=5.csv': {'state': 'crack', 'wind_speed': 5.0},\n",
        "    'Erosion Fault state-Vw=5.csv': {'state': 'erosion', 'wind_speed': 5.0},\n",
        "    'unbalance fault state-Vw=5.csv': {'state': 'unbalance', 'wind_speed': 5.0},\n",
        "    'twist fault when Vw=5.xlsx': {'state': 'twist', 'wind_speed': 5.0},\n",
        "    'H-for Vw=1.3.csv': {'state': 'healthy', 'wind_speed': 1.3},\n",
        "    'Crack State-Vw=1.3.csv': {'state': 'crack', 'wind_speed': 1.3},\n",
        "    'Erosion fault state-Vw=1.3.csv': {'state': 'erosion', 'wind_speed': 1.3},\n",
        "    'UnbalanceState-Vw=1.3.csv': {'state': 'unbalance', 'wind_speed': 1.3},\n",
        "    'twist fault when Vw=1.3.xlsx': {'state': 'twist', 'wind_speed': 1.3},\n",
        "    'H for Vw=2.3.csv': {'state': 'healthy', 'wind_speed': 2.3},\n",
        "    'unbalance fault state=Vw=2.3.csv': {'state': 'unbalance', 'wind_speed': 2.3},\n",
        "    'H-for-Vw=3.2.csv': {'state': 'healthy', 'wind_speed': 3.2},\n",
        "    'twist faultwhenVw=3.2.xlsx': {'state': 'twist', 'wind_speed': 3.2},\n",
        "    'H-for-Vw=3.7.csv': {'state': 'healthy', 'wind_speed': 3.7},\n",
        "    'H-for-Vw=4.5.csv': {'state': 'healthy', 'wind_speed': 4.5},\n",
        "    'crack fault state-Vw=4.5.csv': {'state': 'crack', 'wind_speed': 4.5},\n",
        "    'H-Vw=5.3.csv': {'state': 'healthy', 'wind_speed': 5.3},\n",
        "    'Crack Fault-Vw=5.4.csv': {'state': 'crack', 'wind_speed': 5.4},\n",
        "    'Erosion fault state-Vw=5.3.csv': {'state': 'erosion', 'wind_speed': 5.3},\n",
        "    'unbalance fault state-Vw=4.7.csv': {'state': 'unbalance', 'wind_speed': 4.7},\n",
        "    'unbalance fault state-Vw=4.2.csv': {'state': 'unbalance', 'wind_speed': 4.2},\n",
        "    'Crack state-Vw=2.8.csv': {'state': 'crack', 'wind_speed': 2.8},\n",
        "    'Erosion fault state-Vw=2.1.csv': {'state': 'erosion', 'wind_speed': 2.1},\n",
        "    'Erosion fault state-Vw=2.8.csv': {'state': 'erosion', 'wind_speed': 2.8},\n",
        "    'unbalnce fault state-Vw=3.csv': {'state': 'unbalance', 'wind_speed': 3.0},\n",
        "    'Erosion Fault State-Vw=4.2.csv': {'state': 'erosion', 'wind_speed': 4.2},\n",
        "    'unbalance fault state-Vw=3.4.csv': {'state': 'unbalance', 'wind_speed': 3.4},\n",
        "    'twist fault when Vw=4.7.xlsx': {'state': 'twist', 'wind_speed': 4.7},\n",
        "    'crack fault-Vw=4.csv': {'state': 'crack', 'wind_speed': 4.0},\n",
        "    'Crack state-Vw=3.3.csv': {'state': 'crack', 'wind_speed': 3.3},\n",
        "    'Erosion fault state-Vw=3.4.csv': {'state': 'erosion', 'wind_speed': 3.4},\n",
        "    'twist fault when Vwind=4.xlsx': {'state': 'twist', 'wind_speed': 4.0},\n",
        "    'twsist faut when Vw=2.xlsx': {'state': 'twist', 'wind_speed': 2.0},\n",
        "    'twist fault when Vw=5.3.xlsx': {'state': 'twist', 'wind_speed': 5.3},\n",
        "    'H-for-Vw=5.csv': {'state': 'healthy', 'wind_speed': 5.0}  # Thêm file bị thiếu\n",
        "}\n",
        "\n",
        "# Lưu file cấu hình nếu chưa tồn tại\n",
        "if not os.path.exists(config_file):\n",
        "    with open(config_file, 'w') as f:\n",
        "        json.dump(file_config, f, indent=4)\n",
        "\n",
        "with tempfile.TemporaryDirectory() as temp_dir:\n",
        "    print(f\"Giải nén file ZIP vào thư mục tạm: {temp_dir}\")\n",
        "    try:\n",
        "        with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
        "            print(\"Nội dung file ZIP:\")\n",
        "            for file in zip_ref.namelist():\n",
        "                print(file)\n",
        "            zip_ref.extractall(temp_dir)\n",
        "    except zipfile.BadZipFile:\n",
        "        raise ValueError(f\"Lỗi: {zip_file} không phải là file ZIP hợp lệ hoặc bị hỏng\")\n",
        "\n",
        "    def find_data_files(directory):\n",
        "        data_files = []\n",
        "        for root, _, files in os.walk(directory):\n",
        "            for file in files:\n",
        "                if file.endswith('.csv') or file.endswith('.xlsx'):\n",
        "                    data_files.append(os.path.join(root, file))\n",
        "        return data_files\n",
        "\n",
        "    data_files = find_data_files(temp_dir)\n",
        "    if not data_files:\n",
        "        raise FileNotFoundError(\"Không tìm thấy file CSV hoặc XLSX trong file ZIP đã giải nén\")\n",
        "\n",
        "    # Đọc file cấu hình\n",
        "    try:\n",
        "        with open(config_file, 'r') as f:\n",
        "            file_config = json.load(f)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Không tìm thấy {config_file}. Sử dụng cấu hình mặc định.\")\n",
        "\n",
        "    dataset = []\n",
        "    for data_file in data_files:\n",
        "        file = os.path.basename(data_file)\n",
        "        print(f\"Xử lý file: {file}\")\n",
        "        try:\n",
        "            if file.endswith('.csv'):\n",
        "                df = pd.read_csv(data_file, sep=';|,', engine='python')\n",
        "            elif file.endswith('.xlsx'):\n",
        "                df = pd.read_excel(data_file)\n",
        "        except Exception as e:\n",
        "            print(f\"Lỗi khi đọc file {file}: {e}\")\n",
        "            continue\n",
        "\n",
        "        print(f\"Các cột trong {file}: {df.columns}\")\n",
        "\n",
        "        # Lấy trạng thái và tốc độ gió từ file cấu hình\n",
        "        config_entry = file_config.get(file, None)\n",
        "        if config_entry is None:\n",
        "            print(f\"File {file} không có trong file cấu hình. Bỏ qua.\")\n",
        "            continue\n",
        "\n",
        "        state = config_entry['state']\n",
        "        wind_speed = config_entry['wind_speed']\n",
        "\n",
        "        # Chuẩn hóa tên cột\n",
        "        column_map = {\n",
        "            'Time - Voltage_1;Amplitude - Voltage_1': 'amplitude',\n",
        "            'Time - sec;Amplitude - g': 'amplitude',\n",
        "            'Amplitude - Voltage_1': 'amplitude',\n",
        "            'Time - Voltage_1': 'time',\n",
        "            'Time - sec': 'time'\n",
        "        }\n",
        "        df = df.rename(columns=column_map)\n",
        "\n",
        "        if 'amplitude' not in df.columns:\n",
        "            for col in df.columns:\n",
        "                if 'Amplitude' in col:\n",
        "                    df['amplitude'] = df[col]\n",
        "                elif ';' in str(df[col].iloc[0]):\n",
        "                    df['amplitude'] = df[col].apply(lambda x: float(x.split(';')[-1]) if isinstance(x, str) else float(x))\n",
        "                    df['time'] = df[col].apply(lambda x: float(x.split(';')[0]) if isinstance(x, str) else np.nan)\n",
        "                    break\n",
        "\n",
        "        if 'amplitude' not in df.columns:\n",
        "            print(f\"Không tìm thấy cột amplitude trong {file}\")\n",
        "            continue\n",
        "\n",
        "        if 'time' not in df.columns:\n",
        "            df['time'] = np.arange(len(df)) / 1000.0\n",
        "\n",
        "        df['state'] = state\n",
        "        df['wind_speed'] = wind_speed\n",
        "        dataset.append(df)\n",
        "\n",
        "if not dataset:\n",
        "    raise ValueError(\"Không có file nào được xử lý thành công. Kiểm tra file cấu hình và dữ liệu.\")\n",
        "\n",
        "full_data = pd.concat(dataset, ignore_index=True)\n",
        "\n",
        "# Bước 2: Tiền xử lý dữ liệu\n",
        "print(\"Các cột trong full_data:\", full_data.columns)\n",
        "print(\"Giá trị thiếu:\\n\", full_data.isna().sum())\n",
        "print(\"Kiểm tra giá trị bất thường trong amplitude:\")\n",
        "print(full_data['amplitude'].describe())\n",
        "print(\"Số giá trị NaN hoặc Inf:\", full_data['amplitude'].isna().sum() + np.isinf(full_data['amplitude']).sum())\n",
        "full_data = full_data.dropna(subset=['amplitude'])  # Loại bỏ các mẫu có amplitude NaN\n",
        "scaler = StandardScaler()\n",
        "full_data['amplitude_scaled'] = scaler.fit_transform(full_data[['amplitude']])\n",
        "\n",
        "# Bước 3: Trích xuất đặc trưng\n",
        "def extract_features(signal, fs=1000):\n",
        "    if isinstance(signal, pd.Series):\n",
        "        signal = signal.to_numpy()\n",
        "    elif not isinstance(signal, np.ndarray):\n",
        "        signal = np.array(signal)\n",
        "\n",
        "    features = {\n",
        "        'mean': signal.mean(),\n",
        "        'std': signal.std(),\n",
        "        'peak': np.abs(signal).max(),\n",
        "        'rms': ((signal ** 2).mean()) ** 0.5,\n",
        "        'skewness': pd.Series(signal).skew(),\n",
        "        'kurtosis': pd.Series(signal).kurt(),\n",
        "        'crest_factor': np.abs(signal).max() / (((signal ** 2).mean()) ** 0.5) if ((signal ** 2).mean()) > 0 else 0,\n",
        "        'signal_entropy': entropy(np.histogram(signal, bins=10, density=True)[0]) if np.sum(signal) > 0 else 0,\n",
        "        'zero_crossing_rate': ((signal[:-1] * signal[1:] < 0).sum()) / len(signal)\n",
        "    }\n",
        "    fft_vals = np.abs(fft(signal))[:len(signal) // 2]\n",
        "    freqs = np.linspace(0, fs / 2, len(fft_vals))\n",
        "    low_freq = fft_vals[(freqs <= 50)]\n",
        "    mid_freq = fft_vals[(freqs > 50) & (freqs <= 200)]\n",
        "    high_freq = fft_vals[(freqs > 200) & (freqs <= 500)]\n",
        "    features['low_freq_energy'] = np.log1p(np.sum(low_freq ** 2)) if len(low_freq) > 0 else 0\n",
        "    features['mid_freq_energy'] = np.log1p(np.sum(mid_freq ** 2)) if len(mid_freq) > 0 else 0\n",
        "    features['high_freq_energy'] = np.log1p(np.sum(high_freq ** 2)) if len(low_freq) > 0 else 0\n",
        "    coeffs = pywt.wavedec(signal, 'db4', level=2)  # Sử dụng db4 và level=2\n",
        "    features['wavelet_energy'] = sum(np.sum(c ** 2) for c in coeffs)\n",
        "    features['wavelet_std'] = np.std(coeffs[-1])\n",
        "    features['approx_energy'] = np.sum(coeffs[0] ** 2)\n",
        "    features['detail_energy_1'] = np.sum(coeffs[1] ** 2)\n",
        "    features['detail_energy_2'] = np.sum(coeffs[2] ** 2)\n",
        "    return features\n",
        "\n",
        "def augment_signal(signal, noise_factor=0.02, shift_max=5, scale_factor_range=(0.9, 1.1)):\n",
        "    noise = np.random.normal(0, noise_factor, len(signal))\n",
        "    noisy_signal = signal + noise\n",
        "    shift = np.random.randint(-shift_max, shift_max)\n",
        "    shifted_signal = np.roll(signal, shift)\n",
        "    scale_factor = np.random.uniform(scale_factor_range[0], scale_factor_range[1])\n",
        "    scaled_signal = signal * scale_factor\n",
        "    return noisy_signal, shifted_signal, scaled_signal\n",
        "\n",
        "# Tự động xác định segment_length\n",
        "def determine_segment_length(group_sizes):\n",
        "    min_samples = min(group_sizes)\n",
        "    if min_samples < 10:\n",
        "        raise ValueError(\"Nhóm nhỏ nhất có ít hơn 10 mẫu, không đủ để trích xuất đặc trưng.\")\n",
        "    elif min_samples < 100:\n",
        "        return 50\n",
        "    elif min_samples < 200:\n",
        "        return 100\n",
        "    else:\n",
        "        return 200\n",
        "\n",
        "features_list = []\n",
        "group_sizes = [len(group) for _, group in full_data.groupby(['state', 'wind_speed'])]\n",
        "segment_length = determine_segment_length(group_sizes)\n",
        "print(f\"Đã chọn segment_length={segment_length} dựa trên kích thước nhóm.\")\n",
        "\n",
        "for (state, wind_speed), group in full_data.groupby(['state', 'wind_speed']):\n",
        "    print(f\"Nhóm {state}, wind_speed={wind_speed}: {len(group)} mẫu\")\n",
        "    for i in range(0, len(group), segment_length):\n",
        "        segment = group['amplitude'].iloc[i:i + segment_length]\n",
        "        if len(segment) >= 10:\n",
        "            if len(segment) < segment_length:\n",
        "                segment = np.pad(segment, (0, segment_length - len(segment)), mode='constant')\n",
        "            features = extract_features(segment)\n",
        "            features['state'] = state\n",
        "            features['wind_speed'] = wind_speed\n",
        "            features_list.append(features)\n",
        "            noisy_segment, shifted_signal, scaled_segment = augment_signal(segment, noise_factor=0.02, shift_max=5)\n",
        "            for aug_segment in [noisy_segment, shifted_signal, scaled_segment]:\n",
        "                features = extract_features(aug_segment)\n",
        "                features['state'] = state\n",
        "                features['wind_speed'] = wind_speed\n",
        "                features_list.append(features)\n",
        "        else:\n",
        "            print(\n",
        "                f\"Bỏ qua đoạn trong nhóm {state}, wind_speed={wind_speed}: chỉ có {len(segment)} mẫu, cần ít nhất 10\")\n",
        "\n",
        "features_df = pd.DataFrame(features_list)\n",
        "print(\"Số mẫu trong features_df:\", len(features_df))\n",
        "if features_df.empty:\n",
        "    raise ValueError(\"Không tạo được đặc trưng nào. Kiểm tra segment_length và số mẫu mỗi nhóm.\")\n",
        "print(\"Phân bố lớp:\\n\", features_df['state'].value_counts())\n",
        "\n",
        "# Biểu đồ khái quát: Phân bố lớp\n",
        "plt.figure(figsize=(8, 6))\n",
        "features_df['state'].value_counts().plot(kind='bar', color='skyblue')\n",
        "plt.xlabel('Trạng thái')\n",
        "plt.ylabel('Số lượng mẫu')\n",
        "plt.title('Phân bố lớp trong tập dữ liệu')\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(output_dir, 'class_distribution.png'))\n",
        "plt.show()\n",
        "\n",
        "# Biểu đồ khái quát: Boxplot cho các đặc trưng quan trọng\n",
        "key_features = ['mean', 'std', 'peak', 'wavelet_energy']\n",
        "plt.figure(figsize=(12, 8))\n",
        "for i, feature in enumerate(key_features, 1):\n",
        "    plt.subplot(2, 2, i)\n",
        "    sns.boxplot(x='state', y=feature, data=features_df)\n",
        "    plt.title(f'Phân bố {feature} theo trạng thái')\n",
        "    plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(output_dir, 'feature_boxplots.png'))\n",
        "plt.show()\n",
        "\n",
        "# Ma trận tương quan trước PCA\n",
        "plt.figure(figsize=(12, 10))\n",
        "corr_matrix = features_df.drop(['state', 'wind_speed'], axis=1).corr()\n",
        "sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', vmin=-1, vmax=1)\n",
        "plt.title('Ma trận tương quan của các đặc trưng trước PCA')\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(output_dir, 'correlation_matrix_pre_pca.png'))\n",
        "plt.show()\n",
        "\n",
        "# Bước 4: Áp dụng PCA để giảm chiều\n",
        "X = features_df.drop(['state', 'wind_speed'], axis=1)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "n_components = min(8, X_scaled.shape[1])  # Giảm xuống 8 thành phần\n",
        "pca = PCA(n_components=n_components)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "print(f\"Đã giảm chiều từ {X_scaled.shape[1]} xuống {X_pca.shape[1]} đặc trưng với PCA.\")\n",
        "print(f\"Tỷ lệ phương sai được giải thích: {sum(pca.explained_variance_ratio_):.4f}\")\n",
        "\n",
        "pca_columns = [f'PC{i+1}' for i in range(X_pca.shape[1])]\n",
        "features_df_pca = pd.DataFrame(X_pca, columns=pca_columns)\n",
        "features_df_pca['state'] = features_df['state']\n",
        "features_df_pca['wind_speed'] = features_df['wind_speed']\n",
        "\n",
        "# Ma trận tương quan sau PCA\n",
        "plt.figure(figsize=(8, 6))\n",
        "corr_matrix_pca = features_df_pca.drop(['state', 'wind_speed'], axis=1).corr()\n",
        "sns.heatmap(corr_matrix_pca, annot=True, fmt='.2f', cmap='coolwarm', vmin=-1, vmax=1)\n",
        "plt.title('Ma trận tương quan của các thành phần PCA')\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(output_dir, 'correlation_matrix_post_pca.png'))\n",
        "plt.show()\n",
        "\n",
        "# Biểu đồ khái quát: Phân tán PCA (PC1 vs PC2)\n",
        "plt.figure(figsize=(10, 8))\n",
        "for state in features_df_pca['state'].unique():\n",
        "    subset = features_df_pca[features_df_pca['state'] == state]\n",
        "    plt.scatter(subset['PC1'], subset['PC2'], label=state, alpha=0.6)\n",
        "plt.xlabel('Thành phần chính 1 (PC1)')\n",
        "plt.ylabel('Thành phần chính 2 (PC2)')\n",
        "plt.title('Phân tán PCA của dữ liệu theo trạng thái')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(output_dir, 'pca_scatter.png'))\n",
        "plt.show()\n",
        "\n",
        "# Bước 5: Trực quan hóa tín hiệu dao động (cho nhiều tốc độ gió)\n",
        "wind_speeds = [1.3, 5.0]\n",
        "for ws in wind_speeds:\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    for state, group in full_data[full_data['wind_speed'] == ws].groupby('state'):\n",
        "        plt.plot(group['time'][:500], group['amplitude'][:500], label=state)\n",
        "    plt.xlabel('Thời gian (s)')\n",
        "    plt.ylabel('Biên độ (g)')\n",
        "    plt.title(f'Tín hiệu dao động tại tốc độ gió {ws} m/s')\n",
        "    plt.legend()\n",
        "    plt.savefig(os.path.join(output_dir, f'vibration_signals_ws_{ws}.png'))\n",
        "    plt.show()\n",
        "\n",
        "# Bước 6: Hàm đánh giá tất cả mô hình\n",
        "def evaluate_all_models(X, y, model_configs, cv=5, test_size=0.15, random_state=42):\n",
        "    results = defaultdict(dict)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=test_size, random_state=random_state, stratify=y\n",
        "    )\n",
        "\n",
        "    le = LabelEncoder()\n",
        "    y_train_encoded = le.fit_transform(y_train)\n",
        "    y_test_encoded = le.transform(y_test)\n",
        "\n",
        "    for model_name, config in model_configs.items():\n",
        "        print(f\"\\nĐang huấn luyện mô hình: {model_name}\")\n",
        "        model = config['model']\n",
        "        param_grid = config['param_grid']\n",
        "\n",
        "        grid_search = GridSearchCV(\n",
        "            model, param_grid, cv=cv, scoring='accuracy', n_jobs=-1, verbose=1\n",
        "        )\n",
        "        grid_search.fit(X_train, y_train_encoded)\n",
        "\n",
        "        results[model_name]['best_score'] = grid_search.best_score_\n",
        "        results[model_name]['best_params'] = grid_search.best_params_\n",
        "        results[model_name]['best_model'] = grid_search.best_estimator_\n",
        "\n",
        "        y_pred = grid_search.best_estimator_.predict(X_test)\n",
        "        report = classification_report(\n",
        "            y_test_encoded, y_pred, target_names=le.classes_, zero_division=0, output_dict=True\n",
        "        )\n",
        "        results[model_name]['classification_report'] = report\n",
        "\n",
        "        # Ma trận nhầm lẫn\n",
        "        cm = confusion_matrix(y_test_encoded, y_pred)\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=le.classes_, yticklabels=le.classes_)\n",
        "        plt.title(f'Ma trận nhầm lẫn - {model_name}')\n",
        "        plt.xlabel('Dự đoán')\n",
        "        plt.ylabel('Thực tế')\n",
        "        plt.savefig(os.path.join(output_dir, f'confusion_matrix_{model_name.lower()}.png'))\n",
        "        plt.show()\n",
        "\n",
        "        if hasattr(grid_search.best_estimator_, 'feature_importances_'):\n",
        "            feature_importance = pd.DataFrame({\n",
        "                'feature': X.columns,\n",
        "                'importance': grid_search.best_estimator_.feature_importances_\n",
        "            }).sort_values(by='importance', ascending=False)\n",
        "            results[model_name]['feature_importance'] = feature_importance\n",
        "        else:\n",
        "            results[model_name]['feature_importance'] = None\n",
        "\n",
        "        print(f\"Điểm kiểm tra chéo tốt nhất ({model_name}): {grid_search.best_score_:.4f}\")\n",
        "        print(f\"Tham số tốt nhất ({model_name}): {grid_search.best_params_}\")\n",
        "        print(f\"Báo cáo phân loại ({model_name}):\\n\", classification_report(\n",
        "            y_test_encoded, y_pred, target_names=le.classes_, zero_division=0\n",
        "        ))\n",
        "        if results[model_name]['feature_importance'] is not None:\n",
        "            print(f\"Tầm quan trọng đặc trưng ({model_name}):\\n\", results[model_name]['feature_importance'])\n",
        "        else:\n",
        "            print(f\"Không thể tính tầm quan trọng đặc trưng cho {model_name}\")\n",
        "\n",
        "        joblib.dump(grid_search.best_estimator_, os.path.join(output_dir, f'wind_turbine_fault_model_{model_name.lower()}.pkl'))\n",
        "        print(f\"Mô hình {model_name} đã được lưu dưới dạng 'wind_turbine_fault_model_{model_name.lower()}.pkl'\")\n",
        "\n",
        "    return results, le, X_test, y_test\n",
        "\n",
        "# Cấu hình các mô hình\n",
        "model_configs = {\n",
        "    'LightGBM': {\n",
        "        'model': lgb.LGBMClassifier(\n",
        "            random_state=42,\n",
        "            class_weight='balanced',\n",
        "            force_col_wise=True,\n",
        "            verbose=-1,\n",
        "            min_child_samples=5,\n",
        "            min_split_gain=0.0\n",
        "        ),\n",
        "        'param_grid': {\n",
        "            'n_estimators': [100, 200, 300],\n",
        "            'max_depth': [5, 7, 10],\n",
        "            'learning_rate': [0.01, 0.1],\n",
        "            'num_leaves': [15, 31, 50],\n",
        "            'subsample': [0.8, 1.0],\n",
        "            'colsample_bytree': [0.8, 1.0]\n",
        "        }\n",
        "    },\n",
        "    'MLP': {\n",
        "        'model': MLPClassifier(random_state=42, max_iter=1000),\n",
        "        'param_grid': {\n",
        "            'hidden_layer_sizes': [(50,), (100,), (50, 50)],\n",
        "            'activation': ['relu', 'tanh'],\n",
        "            'learning_rate_init': [0.001, 0.01],\n",
        "            'alpha': [0.0001, 0.001]\n",
        "        }\n",
        "    },\n",
        "    'SVM': {\n",
        "        'model': SVC(random_state=42, class_weight='balanced'),\n",
        "        'param_grid': {\n",
        "            'C': [0.01, 0.1, 1, 10, 100],\n",
        "            'kernel': ['rbf', 'linear', 'poly'],\n",
        "            'gamma': [0.01, 0.1, 'scale'],\n",
        "            'degree': [2, 3]\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "# Bước 7: Đánh giá tất cả mô hình với dữ liệu PCA\n",
        "X_pca = features_df_pca.drop(['state', 'wind_speed'], axis=1)\n",
        "y = features_df_pca['state']\n",
        "results, label_encoder, X_test, y_test = evaluate_all_models(X_pca, y, model_configs)\n",
        "\n",
        "# Bước 8: So sánh hiệu suất\n",
        "print(\"\\nSo sánh hiệu suất các mô hình:\")\n",
        "comparison = pd.DataFrame({\n",
        "    'Model': [],\n",
        "    'Cross-Validation Accuracy': [],\n",
        "    'Test Accuracy': [],\n",
        "    'Macro F1-Score': []\n",
        "})\n",
        "for model_name, result in results.items():\n",
        "    comparison = pd.concat([comparison, pd.DataFrame({\n",
        "        'Model': [model_name],\n",
        "        'Cross-Validation Accuracy': [result['best_score']],\n",
        "        'Test Accuracy': [result['classification_report']['accuracy']],\n",
        "        'Macro F1-Score': [result['classification_report']['macro avg']['f1-score']]\n",
        "    })], ignore_index=True)\n",
        "\n",
        "comparison = comparison.sort_values(by='Test Accuracy', ascending=False)\n",
        "print(comparison)\n",
        "comparison.to_csv(os.path.join(output_dir, 'model_comparison.csv'), index=False)\n",
        "\n",
        "# Bước 9: Vẽ biểu đồ Cross-Validation Accuracy\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.bar(comparison['Model'], comparison['Cross-Validation Accuracy'], color=['#1f77b4', '#ff7f0e', '#2ca02c'])\n",
        "plt.xlabel('Mô hình')\n",
        "plt.ylabel('Cross-Validation Accuracy')\n",
        "plt.title('So sánh Cross-Validation Accuracy của các mô hình')\n",
        "plt.ylim(0, 1)\n",
        "for i, v in enumerate(comparison['Cross-Validation Accuracy']):\n",
        "    plt.text(i, v + 0.01, f'{v:.4f}', ha='center', va='bottom')\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(output_dir, 'cross_validation_accuracy.png'))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bhPpWTZ_yykv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "outputId": "e9f8012b-7d79-423e-9021-4fedede2a477"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'pywt'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-ee747b0b08e8>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfft\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfft\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpywt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecomposition\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPCA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pywt'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.fft import fft\n",
        "import pywt\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.decomposition import PCA\n",
        "import joblib\n",
        "from scipy.stats import entropy\n",
        "import os\n",
        "from collections import Counter\n",
        "import warnings\n",
        "\n",
        "# Tắt các cảnh báo không cần thiết\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "# Thư mục chứa mô hình và tham số đã lưu\n",
        "output_dir = 'model_outputs'\n",
        "\n",
        "# Hàm trích xuất đặc trưng (giữ nguyên từ mã gốc)\n",
        "def extract_features(signal, fs=1000):\n",
        "    if isinstance(signal, pd.Series):\n",
        "        signal = signal.to_numpy()\n",
        "    elif not isinstance(signal, np.ndarray):\n",
        "        signal = np.array(signal)\n",
        "\n",
        "    features = {\n",
        "        'mean': signal.mean(),\n",
        "        'std': signal.std(),\n",
        "        'peak': np.abs(signal).max(),\n",
        "        'rms': ((signal ** 2).mean()) ** 0.5,\n",
        "        'skewness': pd.Series(signal).skew(),\n",
        "        'kurtosis': pd.Series(signal).kurt(),\n",
        "        'crest_factor': np.abs(signal).max() / (((signal ** 2).mean()) ** 0.5) if ((signal ** 2).mean()) > 0 else 0,\n",
        "        'signal_entropy': entropy(np.histogram(signal, bins=10, density=True)[0]) if np.sum(signal) > 0 else 0,\n",
        "        'zero_crossing_rate': ((signal[:-1] * signal[1:] < 0).sum()) / len(signal)\n",
        "    }\n",
        "    fft_vals = np.abs(fft(signal))[:len(signal) // 2]\n",
        "    freqs = np.linspace(0, fs / 2, len(fft_vals))\n",
        "    low_freq = fft_vals[(freqs <= 50)]\n",
        "    mid_freq = fft_vals[(freqs > 50) & (freqs <= 200)]\n",
        "    high_freq = fft_vals[(freqs > 200) & (freqs <= 500)]\n",
        "    features['low_freq_energy'] = np.log1p(np.sum(low_freq ** 2)) if len(low_freq) > 0 else 0\n",
        "    features['mid_freq_energy'] = np.log1p(np.sum(mid_freq ** 2)) if len(mid_freq) > 0 else 0\n",
        "    features['high_freq_energy'] = np.log1p(np.sum(high_freq ** 2)) if len(low_freq) > 0 else 0\n",
        "    coeffs = pywt.wavedec(signal, 'db4', level=2)\n",
        "    features['wavelet_energy'] = sum(np.sum(c ** 2) for c in coeffs)\n",
        "    features['wavelet_std'] = np.std(coeffs[-1])\n",
        "    features['approx_energy'] = np.sum(coeffs[0] ** 2)\n",
        "    features['detail_energy_1'] = np.sum(coeffs[1] ** 2)\n",
        "    features['detail_energy_2'] = np.sum(coeffs[2] ** 2)\n",
        "    return features\n",
        "\n",
        "# Hàm xử lý và dự đoán\n",
        "def predict_fault(signal=None, csv_file=None, segment_length=200):\n",
        "    # Kiểm tra input\n",
        "    if signal is None and csv_file is None:\n",
        "        # Tạo tín hiệu ngẫu nhiên nếu không có input\n",
        "        signal = np.random.normal(0, 1, segment_length)\n",
        "        print(\"Không có tín hiệu hoặc file CSV được cung cấp. Tạo tín hiệu ngẫu nhiên.\")\n",
        "\n",
        "    # Tải các tham số và mô hình đã lưu\n",
        "    try:\n",
        "        scaler = joblib.load(os.path.join(output_dir, 'scaler.pkl'))\n",
        "        pca = joblib.load(os.path.join(output_dir, 'pca.pkl'))\n",
        "        label_encoder = joblib.load(os.path.join(output_dir, 'label_encoder.pkl'))\n",
        "        models = {\n",
        "            'LightGBM': joblib.load(os.path.join(output_dir, 'wind_turbine_fault_model_lightgbm.pkl')),\n",
        "            'MLP': joblib.load(os.path.join(output_dir, 'wind_turbine_fault_model_mlp.pkl')),\n",
        "            'SVM': joblib.load(os.path.join(output_dir, 'wind_turbine_fault_model_svm.pkl'))\n",
        "        }\n",
        "    except FileNotFoundError as e:\n",
        "        raise FileNotFoundError(f\"Không tìm thấy file mô hình hoặc tham số: {e}. Vui lòng chạy mã huấn luyện trước.\")\n",
        "\n",
        "    # Xử lý dữ liệu đầu vào\n",
        "    if csv_file:\n",
        "        try:\n",
        "            df = pd.read_csv(csv_file, sep=';|,', engine='python')\n",
        "            if 'amplitude' not in df.columns:\n",
        "                for col in df.columns:\n",
        "                    if 'Amplitude' in col or 'amplitude' in col.lower():\n",
        "                        df['amplitude'] = df[col]\n",
        "                        break\n",
        "                    elif ';' in str(df[col].iloc[0]):\n",
        "                        df['amplitude'] = df[col].apply(lambda x: float(x.split(';')[-1]) if isinstance(x, str) else float(x))\n",
        "                        break\n",
        "            if 'amplitude' not in df.columns:\n",
        "                raise ValueError(\"File CSV không chứa cột 'amplitude'.\")\n",
        "            signal = df['amplitude'].to_numpy()\n",
        "        except Exception as e:\n",
        "            raise ValueError(f\"Lỗi khi đọc file CSV: {e}\")\n",
        "\n",
        "    # Chuyển tín hiệu thành numpy array\n",
        "    signal = np.array(signal, dtype=float)\n",
        "    if len(signal) < 10:\n",
        "        raise ValueError(\"Tín hiệu quá ngắn, cần ít nhất 10 mẫu.\")\n",
        "\n",
        "    # Chia tín hiệu thành các đoạn\n",
        "    segments = []\n",
        "    if len(signal) < segment_length:\n",
        "        # Đệm tín hiệu ngắn bằng 0\n",
        "        signal = np.pad(signal, (0, segment_length - len(signal)), mode='constant')\n",
        "        segments.append(signal)\n",
        "    else:\n",
        "        for i in range(0, len(signal), segment_length):\n",
        "            segment = signal[i:i + segment_length]\n",
        "            if len(segment) >= 10:\n",
        "                if len(segment) < segment_length:\n",
        "                    segment = np.pad(segment, (0, segment_length - len(segment)), mode='constant')\n",
        "                segments.append(segment)\n",
        "\n",
        "    if not segments:\n",
        "        raise ValueError(\"Không tạo được đoạn tín hiệu nào hợp lệ.\")\n",
        "\n",
        "    # Trích xuất đặc trưng\n",
        "    features_list = []\n",
        "    for segment in segments:\n",
        "        features = extract_features(segment)\n",
        "        features_list.append(features)\n",
        "\n",
        "    features_df = pd.DataFrame(features_list)\n",
        "\n",
        "    # Chuẩn hóa và giảm chiều\n",
        "    X_scaled = scaler.transform(features_df)\n",
        "    X_pca = pca.transform(X_scaled)\n",
        "\n",
        "    # Dự đoán\n",
        "    predictions = {}\n",
        "    all_labels = []\n",
        "    for model_name, model in models.items():\n",
        "        y_pred = model.predict(X_pca)\n",
        "        y_pred_labels = label_encoder.inverse_transform(y_pred)\n",
        "        all_labels.extend(y_pred_labels)\n",
        "\n",
        "        try:\n",
        "            y_proba = model.predict_proba(X_pca)\n",
        "            proba_dict = {label: proba for label, proba in zip(label_encoder.classes_, y_proba[0])}\n",
        "        except AttributeError:\n",
        "            proba_dict = None\n",
        "\n",
        "        predictions[model_name] = {\n",
        "            'labels': y_pred_labels,\n",
        "            'probabilities': proba_dict\n",
        "        }\n",
        "\n",
        "    # Kết hợp dự đoán bằng voting\n",
        "    vote_counts = Counter(all_labels)\n",
        "    ensemble_label = vote_counts.most_common(1)[0][0]\n",
        "    predictions['Ensemble'] = {'labels': [ensemble_label], 'probabilities': None}\n",
        "\n",
        "    return predictions\n",
        "\n",
        "# Hàm hiển thị kết quả\n",
        "def display_predictions(predictions):\n",
        "    print(\"\\nKết quả dự đoán lỗi cánh tuabin gió:\")\n",
        "    for model_name, pred in predictions.items():\n",
        "        print(f\"\\nMô hình {model_name}:\")\n",
        "        print(f\"Loại lỗi dự đoán: {pred['labels'][0]}\")\n",
        "        if pred['probabilities']:\n",
        "            print(\"Xác suất cho từng lớp:\")\n",
        "            for label, proba in pred['probabilities'].items():\n",
        "                print(f\"  {label}: {proba:.4f}\")\n",
        "\n",
        "# Lưu các tham số cần thiết từ mã huấn luyện\n",
        "def save_training_params(scaler, pca, label_encoder):\n",
        "    joblib.dump(scaler, os.path.join(output_dir, 'scaler.pkl'))\n",
        "    joblib.dump(pca, os.path.join(output_dir, 'pca.pkl'))\n",
        "    joblib.dump(label_encoder, os.path.join(output_dir, 'label_encoder.pkl'))\n",
        "    print(\"Đã lưu các tham số: scaler, pca, label_encoder.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "3j1QAlzckZNs"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ví dụ sử dụng\n",
        "if __name__ == \"__main__\":\n",
        "    # Ví dụ 1: Nhập danh sách số liệu\n",
        "    sample_signal = [\n",
        "    -0.001167, -0.003504, -0.002822, 0.000201, 0.000636, 0.006501, -0.003403, -0.006783, -0.00208,\n",
        "    -0.000371, 0.001474, -0.001588, 0.004165, 0.000814, 0.000524, -0.00561, 0.001716, 0.003939,\n",
        "    -0.001839, -0.014185, 0.001023, -0.004787, -0.001749, -0.003032, 0.004793, 0.00584, -0.004466,\n",
        "    0.005212, -0.001488, 0.003295, 0.000588, 0.003698, -7.82e-05, 0.00236, 0.004229, 0.000814,\n",
        "    -0.001688, -0.002681, -0.002691, -0.003042, 0.016441, -0.006412, 0.001474, -0.004446, 0.007919,\n",
        "    0.005727, 0.007419, -0.002832, 0.002022, 0.009417, -0.003454, 0.00054, 0.002151, 0.000975,\n",
        "    0.005164, 0.001571, -0.003183, -0.004326, 0.000996, -0.007064, 0.001974, -0.003965, -0.00228,\n",
        "    -0.00227, 0.007113, 0.01077, -0.001378, 0.005599, -0.004126, -0.003263, -0.001979, -0.001237,\n",
        "    0.002119, -0.003574, 0.004245, 0.004052, -0.007967, -0.001859, 0.011141, -0.00215, 0.002183,\n",
        "    -0.001368, -0.001719, 0.002634, 0.000298, -0.001678, -0.002822, -0.006693, -0.003393, -0.003995,\n",
        "    -0.003243, -0.001979, 0.00729, 0.008096, 0.001829, -0.005941, 0.00083, 0.000427, 0.001458,\n",
        "    -0.001187, 0.000508, 0.001974, 0.000105, -0.00233, -0.002601, -0.004406, -0.002621, 0.012623,\n",
        "    -0.006232, -0.002661, -0.002701, 0.000801, 0.007403, 0.001587, 0.006533, 0.014363, 0.006759,\n",
        "    -0.011838, -0.001528, 0.001764, 0.001974, 0.001313, 0.001281, -0.004216, 0.002683, -0.004517,\n",
        "    -0.01267, -0.007204, -0.003875, -0.003042, -0.00233, -0.001368, 0.0041, 0.010416, 0.002844,\n",
        "    -0.003273, 0.011109, 0.00236, -0.000176, 0.004197, 0.0012, 0.00112, -0.001999, 0.002538,\n",
        "    -0.005931, -0.007104, -0.00562, 0.011527, -0.001919, -0.006573, -0.001979, -0.001368, 0.001941,\n",
        "    0.005695, 0.000749, 0.003859, 0.007661, -0.007395, 0.004052, 0.011527, 0.004535, 0.001265,\n",
        "    0.004245, -0.005189, 0.004326, -0.001458, -0.001568, -0.001769, -0.004667, 0.000234, -0.005389,\n",
        "    0.000234, -0.00213, 0.013686, 0.006726, 0.000996, -0.004406, 0.016989, 0.018326, 0.00033,\n",
        "    -7.82e-05, 0.000943, 0.002296, 0.001506, -0.007706, -0.007596, 0.001168, -0.003243, -0.008689,\n",
        "    -0.004446, -0.003333, 0.000588, -0.001428, 0.009659, 0.004213, 0.001909, 0.006404, -0.004146,\n",
        "    -0.005479, 0.001925, 0.003166, 0.002666, 0.001909, -0.002912, -0.002792, 0.005099, -0.001919,\n",
        "    -0.007746, 0.000312, -0.002411, -0.001057, -0.002491, -0.000273, 0.000491, 0.006275, 0.00584,\n",
        "    -0.00209, 0.010931, 0.01309, -0.003664, 0.000862, 0.000798, 0.000943, 0.002908, -0.003524,\n",
        "    -0.002892, -0.004918, 0.002747, -0.008308, -0.001317, 0.002844, -0.001979, 0.000282, 0.002828,\n",
        "    0.008402, 0.001088, 0.014218, 0.005937, -0.009491, -0.003133, 0.00816, -0.001187, 0.000604,\n",
        "    0.001587, 0.001845, -0.003283, -0.010725, -0.010664, 0.001039, 0.001136, -0.004767, -0.002701,\n",
        "    -0.001909, 0.002215, 0.01367, 0.000991, 0.000117, 0.014411, 0.001958, -0.001558, -0.00216,\n",
        "    0.004696, 0.0017, 0.001281, -0.003674, -0.003544, -0.004697, -0.001628, -0.004657, -0.011958,\n",
        "    -0.006422, -0.00212, -0.001588, 0.002425, 0.001974, 0.008418, 0.014572, 0.01533, -0.008227,\n",
        "    -0.006542, 0.012124, -0.001678, 0.002602, -0.000957, 0.00489, -0.003263, 0.004729, -0.013041,\n",
        "    -0.002992, 0.019051, -0.003925, -0.004115, -0.00233, -0.001899, 0.007774, 0.005792, -0.002531,\n",
        "    -0.001678, 0.004825, 0.015185, -0.005098, 0.002924, -0.001498, 0.005776, -0.00214, 0.001941,\n",
        "    -0.001668, -0.003133, 0.003617, -0.011246, -0.006001, -0.003955, -0.001638, -0.00229, 0.006694,\n",
        "    0.005969, 0.006597, 0.009546, 0.001506, 0.00468, 0.001458, 0.002328, 0.000572, 0.001716,\n",
        "    -0.003604, -0.003143, -0.00231, -0.002822, 0.005212, 0.01098, 0.003182, -0.003654, -0.002782,\n",
        "    -0.002661, 0.005969, 0.008418, 0.00112, -0.0022, -0.002932, 0.002521, 0.001007, -0.003062,\n",
        "    0.000605, 0.002248, 0.004777, -0.003403, -0.00215, -0.010303, 0.001023, -0.009972, -0.012219,\n",
        "    -0.002611, -0.003133, -0.001678, 0.001184, 0.005711, 0.003085, 0.005776, -0.003805, -0.006723,\n",
        "    4.03e-05, 0.006114, -0.001368, 0.002151, -0.00205, -0.001889, 0.003279, -0.003885, -0.004166,\n",
        "    0.002715, 0.005937, -0.00224, -0.004216, -0.0021, 0.000996, 0.012462, -0.002401, 0.002054,\n",
        "    0.0041, 0.00721, 0.005389, -0.004426, 0.001893, 0.001088, 0.001233, 0.001458, -0.004527,\n",
        "    -0.003383, 0.003424, -0.009812, -0.015639, -0.0022, -0.003323, 0.001104, -0.003103, 0.005067,\n",
        "    0.005035, 0.016264, 0.003585, -0.00219, 0.017537, 0.003166, 0.003214, -0.001558, 0.001523,\n",
        "    0.003166, 0.001313, -0.004136, -0.001207, -0.003383, -0.004136, 0.000169, -0.005871, -0.003383,\n",
        "    -0.001989, -0.001037, 0.004874, -0.004737, 0.00294, 0.011544, -0.001829, 0.001796, 0.002924,\n",
        "    0.000137, 0.002795, -0.001959, -0.006091, 0.001055, -0.00226, -0.003062, -0.017594, -0.005008,\n",
        "    -0.002832, 0.00149, -0.003815, 0.003714, 0.010657, 0.012849, 0.006162, 0.002634, 0.007242,\n",
        "    0.001877, 0.006098, 0.000218, 0.00286, -0.002862, 0.000701, -0.010203, -0.010755, -0.008298,\n",
        "    0.01475, 0.001361, -0.007676, -0.000762, -0.003093, 0.001555, 0.003359, 0.00497, 0.004954\n",
        "]\n",
        "    try:\n",
        "        predictions = predict_fault(signal=sample_signal)\n",
        "        display_predictions(predictions)\n",
        "    except Exception as e:\n",
        "        print(f\"Lỗi khi dự đoán: {e}\")\n",
        "\n",
        "    # Ví dụ 2: Nhập từ file CSV\n",
        "    # csv_file = \"your_signal.csv\"\n",
        "    # try:\n",
        "    #     predictions = predict_fault(csv_file=csv_file)\n",
        "    #     display_predictions(predictions)\n",
        "    # except Exception as e:\n",
        "    #     print(f\"Lỗi khi dự đoán: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQ3_C_SEku1C",
        "outputId": "422f246c-61a6-4c5d-b227-98dde220d73e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lỗi khi dự đoán: Không tìm thấy file mô hình hoặc tham số: [Errno 2] No such file or directory: 'model_outputs/wind_turbine_fault_model_lightgbm.pkl'. Vui lòng chạy mã huấn luyện trước.\n"
          ]
        }
      ]
    }
  ]
}